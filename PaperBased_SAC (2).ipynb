{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "import json\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import time\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "class data_cls:\n",
    "    def __init__(self, train_test, **kwargs):\n",
    "        col_names = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n",
    "                     \"dst_bytes\", \"land_f\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
    "                     \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\",\n",
    "                     \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "                     \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\n",
    "                     \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
    "                     \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
    "                     \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
    "                     \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "                     \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"labels\", \"dificulty\"]\n",
    "        self.index = 0\n",
    "        # Data formated path and test path.\n",
    "        self.loaded = False\n",
    "        self.train_test = train_test\n",
    "        self.train_path = kwargs.get('train_path', './KDDTrain+.txt')\n",
    "        self.test_path = kwargs.get('test_path','./KDDTest+.csv')\n",
    "\n",
    "        self.formated_train_path = kwargs.get('formated_train_path',\n",
    "                                              \"formated_train_adv.data\")\n",
    "        self.formated_test_path = kwargs.get('formated_test_path',\n",
    "                                             \"formated_test_adv.data\")\n",
    "\n",
    "        self.attack_types = ['normal', 'DoS', 'Probe', 'R2L', 'U2R']\n",
    "        self.attack_names = []\n",
    "        self.attack_map = {'normal': 'normal',\n",
    "\n",
    "                           'back': 'DoS',\n",
    "                           'land': 'DoS',\n",
    "                           'neptune': 'DoS',\n",
    "                           'pod': 'DoS',\n",
    "                           'smurf': 'DoS',\n",
    "                           'teardrop': 'DoS',\n",
    "                           'mailbomb': 'DoS',\n",
    "                           'apache2': 'DoS',\n",
    "                           'processtable': 'DoS',\n",
    "                           'udpstorm': 'DoS',\n",
    "\n",
    "                           'ipsweep': 'Probe',\n",
    "                           'nmap': 'Probe',\n",
    "                           'portsweep': 'Probe',\n",
    "                           'satan': 'Probe',\n",
    "                           'mscan': 'Probe',\n",
    "                           'saint': 'Probe',\n",
    "\n",
    "                           'ftp_write': 'R2L',\n",
    "                           'guess_passwd': 'R2L',\n",
    "                           'imap': 'R2L',\n",
    "                           'multihop': 'R2L',\n",
    "                           'phf': 'R2L',\n",
    "                           'spy': 'R2L',\n",
    "                           'warezclient': 'R2L',\n",
    "                           'warezmaster': 'R2L',\n",
    "                           'sendmail': 'R2L',\n",
    "                           'named': 'R2L',\n",
    "                           'snmpgetattack': 'R2L',\n",
    "                           'snmpguess': 'R2L',\n",
    "                           'xlock': 'R2L',\n",
    "                           'xsnoop': 'R2L',\n",
    "                           'worm': 'R2L',\n",
    "\n",
    "                           'buffer_overflow': 'U2R',\n",
    "                           'loadmodule': 'U2R',\n",
    "                           'perl': 'U2R',\n",
    "                           'rootkit': 'U2R',\n",
    "                           'httptunnel': 'U2R',\n",
    "                           'ps': 'U2R',\n",
    "                           'sqlattack': 'U2R',\n",
    "                           'xterm': 'U2R'\n",
    "                           }\n",
    "        self.all_attack_names = list(self.attack_map.keys())\n",
    "\n",
    "        formated = False\n",
    "\n",
    "        # Test formated data exists\n",
    "        if os.path.exists(self.formated_train_path) and os.path.exists(self.formated_test_path):\n",
    "            formated = True\n",
    "\n",
    "        self.formated_dir = \"../datasets/formated/\"\n",
    "        if not os.path.exists(self.formated_dir):\n",
    "            os.makedirs(self.formated_dir)\n",
    "\n",
    "        # If it does not exist, it's needed to format the data\n",
    "        if not formated:\n",
    "            ''' Formating the dataset for ready-2-use data'''\n",
    "            self.df = pd.read_csv(self.train_path, sep=',', names=col_names, index_col=False)\n",
    "            self.df = self.df[:10000]\n",
    "            if 'dificulty' in self.df.columns:\n",
    "                self.df.drop('dificulty', axis=1, inplace=True)  # in case of difficulty\n",
    "\n",
    "            data2 = pd.read_csv(self.test_path, sep=',', names=col_names, index_col=False)\n",
    "            data2 = data2[:3000]\n",
    "            if 'dificulty' in data2:\n",
    "                del (data2['dificulty'])\n",
    "            train_indx = self.df.shape[0]\n",
    "            frames = [self.df, data2]\n",
    "            self.df = pd.concat(frames)\n",
    "\n",
    "            # Dataframe processing\n",
    "            self.df = pd.concat([self.df.drop('protocol_type', axis=1), pd.get_dummies(self.df['protocol_type'])],\n",
    "                                axis=1)\n",
    "            self.df = pd.concat([self.df.drop('service', axis=1), pd.get_dummies(self.df['service'])], axis=1)\n",
    "            self.df = pd.concat([self.df.drop('flag', axis=1), pd.get_dummies(self.df['flag'])], axis=1)\n",
    "\n",
    "            # 1 if ``su root'' command attempted; 0 otherwise\n",
    "            self.df['su_attempted'] = self.df['su_attempted'].replace(2.0, 0.0)\n",
    "\n",
    "            # One hot encoding for labels\n",
    "            self.df = pd.concat([self.df.drop('labels', axis=1),\n",
    "                                 pd.get_dummies(self.df['labels'])], axis=1)\n",
    "\n",
    "            # Normalization of the df\n",
    "            # normalized_df=(df-df.mean())/df.std() - MinMax Scaling was done in below code\n",
    "            for indx, dtype in self.df.dtypes.items():\n",
    "                if dtype == 'float64' or dtype == 'int64':\n",
    "                    if self.df[indx].max() == 0 and self.df[indx].min() == 0:\n",
    "                        self.df[indx] = 0\n",
    "                    else:\n",
    "                        self.df[indx] = (self.df[indx] - self.df[indx].min()) / (\n",
    "                                    self.df[indx].max() - self.df[indx].min())\n",
    "\n",
    "            # Save data\n",
    "            test_df = self.df.iloc[train_indx:self.df.shape[0]]\n",
    "            test_df = shuffle(test_df, random_state=np.random.randint(0, 100))\n",
    "            self.df = self.df[:train_indx]\n",
    "            self.df = shuffle(self.df, random_state=np.random.randint(0, 100))\n",
    "            test_df.to_csv(self.formated_test_path, sep=',', index=False)\n",
    "            self.df.to_csv(self.formated_train_path, sep=',', index=False)\n",
    "\n",
    "            # Create a list with the existent attacks in the df\n",
    "            for att in self.attack_map:\n",
    "                if att in self.df.columns:\n",
    "                    # Add only if there is exist at least 1\n",
    "                    if np.sum(self.df[att].values) > 1:\n",
    "                        self.attack_names.append(att)\n",
    "\n",
    "    def get_shape(self):\n",
    "        if self.loaded is False:\n",
    "            self._load_df()\n",
    "\n",
    "        self.data_shape = self.df.shape\n",
    "        # stata + labels\n",
    "        return self.data_shape\n",
    "\n",
    "    ''' Get n-rows from loaded data\n",
    "        The dataset must be loaded in RAM\n",
    "    '''\n",
    "\n",
    "    def get_batch(self, batch_size=100):\n",
    "        if self.loaded is False:\n",
    "            self._load_df()\n",
    "\n",
    "        # Read the df rows\n",
    "        indexes = list(range(self.index, self.index + batch_size))\n",
    "        if max(indexes) > self.data_shape[0] - 1:\n",
    "            dif = max(indexes) - self.data_shape[0]\n",
    "            indexes[len(indexes) - dif - 1:len(indexes)] = list(range(dif + 1))\n",
    "            self.index = batch_size - dif\n",
    "            batch = self.df.iloc[indexes]\n",
    "        else:\n",
    "            batch = self.df.iloc[indexes]\n",
    "            self.index += batch_size\n",
    "\n",
    "        labels = batch[self.attack_names]\n",
    "\n",
    "        #batch = batch.drop(self.all_attack_names, axis=1)\n",
    "\n",
    "        columns_to_drop = [col for col in self.all_attack_names if col in batch.columns]\n",
    "        # Drop only existing columns\n",
    "        batch = batch.drop(columns_to_drop, axis=1)\n",
    "\n",
    "        return batch, labels\n",
    "\n",
    "    def get_full(self):\n",
    "        if self.loaded is False:\n",
    "            self._load_df()\n",
    "\n",
    "        labels = self.df[self.attack_names]\n",
    "\n",
    "        batch = self.df.drop(self.all_attack_names, axis=1)\n",
    "\n",
    "        return batch, labels\n",
    "\n",
    "    def _load_df(self):\n",
    "        if self.train_test == 'train':\n",
    "            self.df = pd.read_csv(self.formated_train_path, sep=',')  # Read again the csv\n",
    "        else:\n",
    "            self.df = pd.read_csv(self.formated_test_path, sep=',')\n",
    "        self.index = np.random.randint(0, self.df.shape[0] - 1, dtype=np.int32)\n",
    "        self.loaded = True\n",
    "        # Create a list with the existent attacks in the df\n",
    "        for att in self.attack_map:\n",
    "            if att in self.df.columns:\n",
    "                # Add only if there is exist at least 1\n",
    "                if np.sum(self.df[att].values) > 1:\n",
    "                    self.attack_names.append(att)\n",
    "        # self.headers = list(self.df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "    \"\"\"\n",
    "    Q-Network Estimator\n",
    "    Represents the global model for the table\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_size, num_actions, hidden_size=100,\n",
    "                 hidden_layers=1, learning_rate=.2, net_name='qv'):\n",
    "        \"\"\"\n",
    "        Initialize the network with the provided shape\n",
    "        \"\"\"\n",
    "        self.net_name = net_name\n",
    "        self.obs_size = obs_size\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # Network arquitecture\n",
    "        self.model = Sequential()\n",
    "        # Add imput layer\n",
    "        self.model.add(Dense(hidden_size, input_shape=(obs_size,),\n",
    "                             activation='relu'))\n",
    "        # Add hidden layers\n",
    "        for layers in range(hidden_layers):\n",
    "            self.model.add(Dense(hidden_size, activation='relu'))\n",
    "        # Add output layer\n",
    "        if self.net_name == 'actor':\n",
    "            self.model.add(Dense(num_actions, activation='softmax'))\n",
    "        else:\n",
    "            self.model.add(Dense(num_actions))\n",
    "\n",
    "        # optimizer = optimizers.SGD(learning_rate)\n",
    "        # optimizer = optimizers.Adam(alpha=learning_rate)\n",
    "        optimizer = optimizers.Adam(0.00025)\n",
    "        # optimizer = optimizers.RMSpropGraves(learning_rate, 0.95, self.momentum, 1e-2)\n",
    "\n",
    "        # Compilation of the model with optimizer and loss\n",
    "        if self.net_name == 'actor':\n",
    "            self.model.compile(loss=sac_loss, optimizer=optimizer)\n",
    "        else:\n",
    "            self.model.compile(loss=tf.keras.losses.mse, optimizer=optimizer)\n",
    "\n",
    "    def predict(self, state, batch_size=1 , verbose = 0):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "        \"\"\"\n",
    "        return self.model.predict(state, batch_size=batch_size , verbose = verbose)\n",
    "\n",
    "    def update(self, states, q):\n",
    "        \"\"\"\n",
    "        Updates the estimator with the targets.\n",
    "\n",
    "        Args:\n",
    "          states: Target states\n",
    "          q: Estimated values\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        loss = self.model.train_on_batch(states, q)\n",
    "        return loss\n",
    "\n",
    "    def copy_model(model):\n",
    "        \"\"\"Returns a copy of a keras model.\"\"\"\n",
    "        model.save('tmp_model')\n",
    "        return tf.keras.models.load_model('tmp_model')\n",
    "\n",
    "def sac_loss(y_true, y_pred):\n",
    "    \"\"\" y_true 是 Q(*, action_n), y_pred 是 pi(*, action_n) \"\"\"\n",
    "    qs = 0.2 * tf.math.xlogy(y_pred, y_pred) - y_pred * y_true\n",
    "    return tf.reduce_sum(qs, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"Implements basic replay memory\"\"\"\n",
    "\n",
    "    def __init__(self, observation_size, max_size):\n",
    "        self.observation_size = observation_size\n",
    "        self.num_observed = 0\n",
    "        self.max_size = max_size\n",
    "        self.samples = {\n",
    "                 'obs'      : np.zeros(self.max_size * 1 * self.observation_size,\n",
    "                                       dtype=np.float32).reshape(self.max_size,self.observation_size),\n",
    "                 'action'   : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
    "                 'reward'   : np.zeros(self.max_size * 1).reshape(self.max_size, 1),\n",
    "                 'terminal' : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
    "               }\n",
    "\n",
    "    def observe(self, state, action, reward, done):\n",
    "        index = self.num_observed % self.max_size\n",
    "        self.samples['obs'][index, :] = state\n",
    "        self.samples['action'][index, :] = action\n",
    "        self.samples['reward'][index, :] = reward\n",
    "        self.samples['terminal'][index, :] = done\n",
    "\n",
    "        self.num_observed += 1\n",
    "\n",
    "    def sample_minibatch(self, minibatch_size):\n",
    "        max_index = min(self.num_observed, self.max_size) - 1\n",
    "        sampled_indices = np.random.randint(max_index, size=minibatch_size)\n",
    "\n",
    "        s      = np.asarray(self.samples['obs'][sampled_indices, :], dtype=np.float32)\n",
    "        s_next = np.asarray(self.samples['obs'][sampled_indices+1, :], dtype=np.float32)\n",
    "\n",
    "        a      = self.samples['action'][sampled_indices].reshape(minibatch_size)\n",
    "        r      = self.samples['reward'][sampled_indices].reshape((minibatch_size, 1))\n",
    "        done   = self.samples['terminal'][sampled_indices].reshape((minibatch_size, 1))\n",
    "\n",
    "        return (s, a, r, s_next, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Reinforcement learning Agent definition\n",
    "'''\n",
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, actions, obs_size, **kwargs):\n",
    "        self.actions = actions\n",
    "        self.ddqn_time = 100\n",
    "        self.ddqn_update = self.ddqn_time\n",
    "        self.num_actions = len(actions)\n",
    "        self.obs_size = obs_size\n",
    "        self.alpha = 0.2\n",
    "        self.net_learning_rate = 0.1\n",
    "        self.epsilon = kwargs.get('epsilon', 1)\n",
    "        self.min_epsilon = kwargs.get('min_epsilon', .1)\n",
    "        self.gamma = kwargs.get('gamma', .001)\n",
    "        self.minibatch_size = kwargs.get('minibatch_size', 2)\n",
    "        self.epoch_length = kwargs.get('epoch_length', 100)\n",
    "        self.ExpRep = kwargs.get('ExpRep', True)\n",
    "        if self.ExpRep:\n",
    "            self.memory = ReplayMemory(self.obs_size, kwargs.get('mem_size', 10))\n",
    "\n",
    "        self.actor_network = QNetwork(self.obs_size, self.num_actions,\n",
    "                                      kwargs.get('hidden_size', 100),\n",
    "                                      kwargs.get('hidden_layers', 1),\n",
    "                                      kwargs.get('learning_rate', .2),\n",
    "                                      net_name='actor')\n",
    "        self.q0_network = QNetwork(self.obs_size, self.num_actions,\n",
    "                                      kwargs.get('hidden_size', 100),\n",
    "                                      kwargs.get('hidden_layers', 1),\n",
    "                                      kwargs.get('learning_rate', .2))\n",
    "        self.q1_network = QNetwork(self.obs_size, self.num_actions,\n",
    "                                             kwargs.get('hidden_size', 100),\n",
    "                                             kwargs.get('hidden_layers', 1),\n",
    "                                             kwargs.get('learning_rate', .2))\n",
    "        self.v_network = QNetwork(self.obs_size, 1,\n",
    "                                             kwargs.get('hidden_size', 100),\n",
    "                                             kwargs.get('hidden_layers', 1),\n",
    "                                             kwargs.get('learning_rate', .2))\n",
    "        self.target_v_network = QNetwork(self.obs_size, 1,\n",
    "                                  kwargs.get('hidden_size', 100),\n",
    "                                  kwargs.get('hidden_layers', 1),\n",
    "                                  kwargs.get('learning_rate', .2))\n",
    "\n",
    "        self.update_target_net(self.target_v_network.model,\n",
    "                               self.v_network.model, self.net_learning_rate)\n",
    "\n",
    "\n",
    "        self.actor_network.model.summary()\n",
    "\n",
    "        self.q0_network.model.summary()\n",
    "\n",
    "        self.target_v_network.model.summary()\n",
    "\n",
    "    def learn(self, states, actions, next_states, rewards, done):\n",
    "        if self.ExpRep:\n",
    "            self.memory.observe(states, actions, rewards, done)\n",
    "        else:\n",
    "            self.states = states\n",
    "            self.actions = actions\n",
    "            self.next_states = next_states\n",
    "            self.rewards = rewards\n",
    "            self.done = done\n",
    "\n",
    "    def update_model(self):\n",
    "        if self.ExpRep:\n",
    "            (states, actions, rewards, next_states, done) = self.memory.sample_minibatch(self.minibatch_size)\n",
    "        else:\n",
    "            states = self.states\n",
    "            rewards = self.rewards\n",
    "            next_states = self.next_states\n",
    "            actions = self.actions\n",
    "            done = self.done\n",
    "\n",
    "        pis = self.actor_network.predict(states)\n",
    "        q0s = self.q0_network.predict(states)\n",
    "        q1s = self.q1_network.predict(states)\n",
    "\n",
    "        # 训练执行者\n",
    "        loss = self.actor_network.model.train_on_batch(states, q0s)\n",
    "\n",
    "        # 训练评论者\n",
    "        q01s = np.minimum(q0s, q1s)\n",
    "        entropic_q01s = pis * q01s - self.alpha * tf.math.xlogy(pis, pis)\n",
    "        v_targets = tf.math.reduce_sum(entropic_q01s, axis=-1)\n",
    "        self.v_network.model.fit(states, v_targets , verbose = 0)\n",
    "\n",
    "        next_vs = self.target_v_network.predict(next_states , verbose = 0)\n",
    "        q_targets = rewards.reshape(q0s[range(self.minibatch_size), actions].shape) + self.gamma * (1. - done.reshape(q0s[range(self.minibatch_size), actions].shape)) * \\\n",
    "                    next_vs[:, 0]\n",
    "        q0s[range(self.minibatch_size), actions] = q_targets\n",
    "        q1s[range(self.minibatch_size), actions] = q_targets\n",
    "        self.q0_network.model.fit(states, q0s , verbose = 0)\n",
    "        self.q1_network.model.fit(states, q1s , verbose = 0)\n",
    "\n",
    "        # 更新目标网络\n",
    "        # self.ddqn_update -= 1\n",
    "        # if self.ddqn_update == 0:\n",
    "        #     self.ddqn_update = self.ddqn_time\n",
    "        #     #            self.target_model_network.model = QNetwork.copy_model(self.model_network.model)\n",
    "        self.update_target_net(self.target_v_network.model,\n",
    "                               self.v_network.model, self.net_learning_rate)\n",
    "        return loss\n",
    "\n",
    "    def act(self, state, policy):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update_target_net(self, target_net, evaluate_net, learning_rate=1.):\n",
    "        target_weights = target_net.get_weights()\n",
    "        evaluate_weights = evaluate_net.get_weights()\n",
    "        average_weights = [(1. - learning_rate) * t + learning_rate * e\n",
    "                for t, e in zip(target_weights, evaluate_weights)]\n",
    "        target_net.set_weights(average_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "class DefenderAgent(Agent):\n",
    "    def __init__(self, actions, obs_size, **kwargs):\n",
    "        super().__init__(actions, obs_size, **kwargs)\n",
    "\n",
    "    def act(self, states):\n",
    "        # Get actions under the policy\n",
    "        probs = self.actor_network.model.predict(states , verbose = 0)[0]\n",
    "        actions = np.random.choice(self.actions, size=1, p=probs)\n",
    "        return actions\n",
    "\n",
    "\n",
    "class AttackAgent(Agent):\n",
    "    def __init__(self, actions, obs_size, **kwargs):\n",
    "        super().__init__(actions, obs_size, **kwargs)\n",
    "\n",
    "    def act(self, states):\n",
    "        # Get actions under the policy\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        probs = self.actor_network.model.predict(states , verbose = 0)[0]\n",
    "        actions = np.random.choice(self.actions, size=1, p=probs)\n",
    "        return actions\n",
    "\n",
    "\n",
    "'''\n",
    "Reinforcement learning Enviroment Definition\n",
    "'''\n",
    "\n",
    "\n",
    "class RLenv(data_cls):\n",
    "    def __init__(self, train_test, **kwargs):\n",
    "        data_cls.__init__(self, train_test, **kwargs)\n",
    "        data_cls._load_df(self)\n",
    "        self.data_shape = data_cls.get_shape(self)\n",
    "        self.batch_size = kwargs.get('batch_size', 1)  # experience replay -> batch = 1\n",
    "        self.iterations_episode = kwargs.get('iterations_episode', 10)\n",
    "        if self.batch_size == 'full':\n",
    "            self.batch_size = int(self.data_shape[0] / iterations_episode)\n",
    "\n",
    "    '''\n",
    "    _update_state: function to update the current state\n",
    "    Returns:\n",
    "        None\n",
    "    Modifies the self parameters involved in the state:\n",
    "        self.state and self.labels\n",
    "    Also modifies the true labels to get learning knowledge\n",
    "    '''\n",
    "\n",
    "    def _update_state(self):\n",
    "        self.states, self.labels = data_cls.get_batch(self)\n",
    "\n",
    "        # Update statistics\n",
    "        self.true_labels += np.sum(self.labels).values\n",
    "\n",
    "    '''\n",
    "    Returns:\n",
    "        + Observation of the enviroment\n",
    "    '''\n",
    "\n",
    "    def reset(self):\n",
    "        # Statistics\n",
    "        self.def_true_labels = np.zeros(len(self.attack_types), dtype=int)\n",
    "        self.def_estimated_labels = np.zeros(len(self.attack_types), dtype=int)\n",
    "        self.att_true_labels = np.zeros(len(self.attack_names), dtype=int)\n",
    "\n",
    "        self.state_numb = 0\n",
    "\n",
    "        data_cls._load_df(self)  # Reload and random index\n",
    "        self.states, self.labels = data_cls.get_batch(self, self.batch_size)\n",
    "\n",
    "        self.total_reward = 0\n",
    "        self.steps_in_episode = 0\n",
    "        return self.states.values\n",
    "\n",
    "    '''\n",
    "    Returns:\n",
    "        State: Next state for the game\n",
    "        Reward: Actual reward\n",
    "        done: If the game ends (no end in this case)\n",
    "\n",
    "    In the adversarial enviroment, it's only needed to return the actual reward\n",
    "    '''\n",
    "\n",
    "    def act(self, defender_actions, attack_actions):\n",
    "        # Clear previous rewards\n",
    "        self.att_reward = np.zeros(len(attack_actions))\n",
    "        self.def_reward = np.zeros(len(defender_actions))\n",
    "\n",
    "        attack = [self.attack_types.index(self.attack_map[self.attack_names[att]]) for att in attack_actions]\n",
    "        # 设置不同的奖励值\n",
    "        if attack[0] in [3,4]:\n",
    "            self.def_reward = (np.asarray(defender_actions) == np.asarray(attack)) * 2\n",
    "            self.att_reward = (np.asarray(defender_actions) != np.asarray(attack)) * 2\n",
    "        # # elif attack[0] in [3]:\n",
    "        # #     self.def_reward = (np.asarray(defender_actions) == np.asarray(attack)) * 2\n",
    "        # #     self.att_reward = (np.asarray(defender_actions) != np.asarray(attack)) * 2\n",
    "        else:\n",
    "            self.def_reward = (np.asarray(defender_actions) == np.asarray(attack)) * 1\n",
    "            self.att_reward = (np.asarray(defender_actions) != np.asarray(attack)) * 1\n",
    "\n",
    "        self.def_estimated_labels += np.bincount(defender_actions, minlength=len(self.attack_types))\n",
    "        # TODO\n",
    "        # list comprehension\n",
    "\n",
    "        for act in attack_actions:\n",
    "            self.def_true_labels[self.attack_types.index(self.attack_map[self.attack_names[act]])] += 1\n",
    "\n",
    "        # Get new state and new true values\n",
    "        attack_actions = attacker_agent.act(self.states)\n",
    "        self.states = env.get_states(attack_actions)\n",
    "\n",
    "        # Done allways false in this continuous task\n",
    "        self.done = np.zeros(len(attack_actions), dtype=bool)\n",
    "\n",
    "        return self.states, self.def_reward, self.att_reward, attack_actions, self.done\n",
    "\n",
    "    '''\n",
    "    Provide the actual states for the selected attacker actions\n",
    "    Parameters:\n",
    "        self:\n",
    "        attacker_actions: optimum attacks selected by the attacker\n",
    "            it can be one of attack_names list and select random of this\n",
    "    Returns:\n",
    "        State: Actual state for the selected attacks\n",
    "    '''\n",
    "\n",
    "    def get_states(self, attacker_actions):\n",
    "        first = True\n",
    "        for attack in attacker_actions:\n",
    "            if first:\n",
    "                minibatch = (self.df[self.df[self.attack_names[attack]] == 1].sample(1))\n",
    "                first = False\n",
    "            else:\n",
    "                minibatch = minibatch.append(self.df[self.df[self.attack_names[attack]] == 1].sample(1))\n",
    "\n",
    "        self.labels = minibatch[self.attack_names]\n",
    "        #minibatch.drop(self.all_attack_names, axis=1, inplace=True)\n",
    "        # in get_states function:\n",
    "        existing_cols = minibatch.columns\n",
    "        cols_to_drop = [c for c in self.all_attack_names if c in existing_cols]\n",
    "        minibatch.drop(cols_to_drop, axis=1, inplace=True)\n",
    "        self.states = minibatch\n",
    "\n",
    "        return self.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    kdd_train = \"./KDDTrain+.csv\"\n",
    "    kdd_test = \"./KDDTest+.csv\"\n",
    "\n",
    "    formated_train_path = \"formated_train_adv.data\"\n",
    "    formated_test_path = \"formated_test_adv.data\"\n",
    "\n",
    "    # Train batch\n",
    "    batch_size = 1\n",
    "    # batch of memory ExpRep\n",
    "    minibatch_size = 180\n",
    "    ExpRep = True\n",
    "\n",
    "    iterations_episode = 180\n",
    "\n",
    "    # Initialization of the enviroment\n",
    "    env = RLenv('train', train_path=kdd_train, test_path=kdd_test,\n",
    "                formated_train_path=formated_train_path,\n",
    "                formated_test_path=formated_test_path, batch_size=batch_size,\n",
    "                iterations_episode=iterations_episode)\n",
    "    # obs_size = size of the state\n",
    "    #obs_size = env.data_shape[1] - len(env.all_attack_names)\n",
    "    obs_size = 116\n",
    "    # num_episodes = int(env.data_shape[0]/(iterations_episode)/10)\n",
    "    num_episodes = 7\n",
    "\n",
    "    '''\n",
    "    Definition for the defensor agent.\n",
    "    '''\n",
    "    defender_valid_actions = list(range(len(env.attack_types)))  # only detect type of attack\n",
    "    defender_num_actions = len(defender_valid_actions)\n",
    "\n",
    "    def_epsilon = 1  # exploration\n",
    "    min_epsilon = 0.01  # min value for exploration\n",
    "    def_gamma = 0.001\n",
    "    def_decay_rate = 0.99\n",
    "\n",
    "    def_hidden_size = 700\n",
    "    def_hidden_layers = 2\n",
    "\n",
    "    def_learning_rate = .2\n",
    "\n",
    "    defender_agent = DefenderAgent(defender_valid_actions, obs_size,\n",
    "                                   epoch_length=iterations_episode,\n",
    "                                   epsilon=def_epsilon,\n",
    "                                   min_epsilon=min_epsilon,\n",
    "                                   decay_rate=def_decay_rate,\n",
    "                                   gamma=def_gamma,\n",
    "                                   hidden_size=def_hidden_size,\n",
    "                                   hidden_layers=def_hidden_layers,\n",
    "                                   minibatch_size=minibatch_size,\n",
    "                                   mem_size=1000,\n",
    "                                   learning_rate=def_learning_rate,\n",
    "                                   ExpRep=ExpRep)\n",
    "    # Pretrained defender\n",
    "    # defender_agent.model_network.model.load_weights(\"models/type_model.h5\")\n",
    "\n",
    "    '''\n",
    "    Definition for the attacker agent.\n",
    "    In this case the exploration is better to be greater\n",
    "    The correlation sould be greater too so gamma bigger\n",
    "    '''\n",
    "    attack_valid_actions = list(range(len(env.attack_names)))\n",
    "    attack_num_actions = len(attack_valid_actions)\n",
    "\n",
    "    att_epsilon = 1\n",
    "    min_epsilon = 0.82  # min value for exploration\n",
    "\n",
    "    att_gamma = 0.001\n",
    "    att_decay_rate = 0.99\n",
    "\n",
    "    att_hidden_layers = 2\n",
    "    att_hidden_size = 100\n",
    "\n",
    "    att_learning_rate = 0.2\n",
    "\n",
    "    attacker_agent = AttackAgent(attack_valid_actions, obs_size,\n",
    "                                 epoch_length=iterations_episode,\n",
    "                                 epsilon=att_epsilon,\n",
    "                                 min_epsilon=min_epsilon,\n",
    "                                 decay_rate=att_decay_rate,\n",
    "                                 gamma=att_gamma,\n",
    "                                 hidden_size=att_hidden_size,\n",
    "                                 hidden_layers=att_hidden_layers,\n",
    "                                 minibatch_size=minibatch_size,\n",
    "                                 mem_size=1000,\n",
    "                                 learning_rate=att_learning_rate,\n",
    "                                 ExpRep=ExpRep)\n",
    "\n",
    "    # Statistics\n",
    "    att_reward_chain = []\n",
    "    def_reward_chain = []\n",
    "    att_loss_chain = []\n",
    "    def_loss_chain = []\n",
    "    def_total_reward_chain = []\n",
    "    att_total_reward_chain = []\n",
    "\n",
    "    # Print parameters\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"Total epoch: {} | Iterations in epoch: {}\"\n",
    "          \"| Minibatch from mem size: {} | Total Samples: {}|\".format(num_episodes,\n",
    "                                                                      iterations_episode, minibatch_size,\n",
    "                                                                      num_episodes * iterations_episode))\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"Dataset shape: {}\".format(env.data_shape))\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"Attacker parameters: Num_actions={} | gamma={} |\"\n",
    "          \" epsilon={} | ANN hidden size={} | \"\n",
    "          \"ANN hidden layers={}|\".format(attack_num_actions,\n",
    "                                         att_gamma, att_epsilon, att_hidden_size,\n",
    "                                         att_hidden_layers))\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"Defense parameters: Num_actions={} | gamma={} | \"\n",
    "          \"epsilon={} | ANN hidden size={} |\"\n",
    "          \" ANN hidden layers={}|\".format(defender_num_actions,\n",
    "                                          def_gamma, def_epsilon, def_hidden_size,\n",
    "                                          def_hidden_layers))\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "    # Main loop\n",
    "    attacks_by_epoch = []\n",
    "    attack_labels_list = []\n",
    "    for epoch in range(num_episodes):\n",
    "        start_time = time.time()\n",
    "        att_loss = 0.\n",
    "        def_loss = 0.\n",
    "        def_total_reward_by_episode = 0\n",
    "        att_total_reward_by_episode = 0\n",
    "        # Reset enviromet, actualize the data batch with random state/attacks\n",
    "        states = env.reset()\n",
    "\n",
    "        # Get actions for actual states following the policy\n",
    "        attack_actions = attacker_agent.act(states)\n",
    "        states = env.get_states(attack_actions)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        attacks_list = []\n",
    "        # Iteration in one episode\n",
    "        for i_iteration in range(iterations_episode):\n",
    "\n",
    "            attacks_list.append(attack_actions[0])\n",
    "            # apply actions, get rewards and new state\n",
    "            act_time = time.time()\n",
    "            defender_actions = defender_agent.act(states)\n",
    "            # Enviroment actuation for this actions\n",
    "            next_states, def_reward, att_reward, next_attack_actions, done = env.act(defender_actions, attack_actions)\n",
    "            # If the epoch*batch_size*iterations_episode is largest than the df\n",
    "\n",
    "            attacker_agent.learn(states, attack_actions, next_states, att_reward, done)\n",
    "            defender_agent.learn(states, defender_actions, next_states, def_reward, done)\n",
    "\n",
    "            act_end_time = time.time()\n",
    "\n",
    "            # Train network, update loss after at least minibatch_learns\n",
    "            if ExpRep and epoch * iterations_episode + i_iteration >= minibatch_size:\n",
    "                def_loss += defender_agent.update_model()\n",
    "                att_loss += attacker_agent.update_model()\n",
    "            elif not ExpRep:\n",
    "                def_loss += defender_agent.update_model()\n",
    "                att_loss += attacker_agent.update_model()\n",
    "\n",
    "            update_end_time = time.time()\n",
    "\n",
    "            # Update the state\n",
    "            states = next_states\n",
    "            attack_actions = next_attack_actions\n",
    "\n",
    "            # Update statistics\n",
    "            def_total_reward_by_episode += np.sum(def_reward, dtype=np.int32)\n",
    "            att_total_reward_by_episode += np.sum(att_reward, dtype=np.int32)\n",
    "\n",
    "        attacks_by_epoch.append(attacks_list)\n",
    "        # Update user view\n",
    "        def_reward_chain.append(def_total_reward_by_episode)\n",
    "        att_reward_chain.append(att_total_reward_by_episode)\n",
    "        def_loss_chain.append(def_loss)\n",
    "        att_loss_chain.append(att_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\"\\r\\n|Epoch {:03d}/{:03d}| time: {:2.2f}|\\r\\n\"\n",
    "              \"|Def Loss {:4.4f} | Def Reward in ep {:03d}|\\r\\n\"\n",
    "              \"|Att Loss {:4.4f} | Att Reward in ep {:03d}|\"\n",
    "              .format(epoch, num_episodes, (end_time - start_time),\n",
    "                      def_loss, def_total_reward_by_episode,\n",
    "                      att_loss, att_total_reward_by_episode))\n",
    "\n",
    "        print(\"|Def Estimated: {}| Att Labels: {}\".format(env.def_estimated_labels,\n",
    "                                                          env.def_true_labels))\n",
    "        attack_labels_list.append(env.def_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Ensure the 'models' directory exists\n",
    "model_dir = 'models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Save trained model weights and architecture\n",
    "try:\n",
    "    defender_agent.actor_network.model.save_weights(f\"{model_dir}/defender_agent_model_0_1.weights.h5\", overwrite=True)\n",
    "    with open(f\"{model_dir}/defender_agent_model_0_1.json\", \"w\") as outfile:\n",
    "        json.dump(defender_agent.actor_network.model.to_json(), outfile)\n",
    "except AttributeError as e:\n",
    "    print(\"Error saving model weights or architecture:\", e)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
